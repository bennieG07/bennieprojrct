{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f42cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import hstack\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb03a94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definitions for cleaning and processing tweets\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean the input text by removing unwanted characters.\"\"\"\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)  # Remove @ mentions\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove links\n",
    "    text = re.sub(r'\\#', '', text)  # Remove hashtags\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5bcd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(content):\n",
    "    \"\"\"Calculates sentiment polarity of the tweet.\"\"\"\n",
    "    return TextBlob(content).sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1aef342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(content):\n",
    "    \"\"\"Lemmatize the text to reduce words to their base form.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_content = re.sub('[^a-zA-Z]', ' ', content).lower().split()\n",
    "    lemmatized_content = [lemmatizer.lemmatize(word) for word in lemmatized_content if word not in stopwords.words('english')]\n",
    "    return ' '.join(lemmatized_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e205be1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    \"\"\"Load the dataset from a CSV file.\"\"\"\n",
    "    return pd.read_csv(file_path).fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d64ec0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_true, y_pred, dataset_name):\n",
    "    \"\"\"Print evaluation metrics for the model.\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Metrics for {dataset_name} Set:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abee54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Load and preprocess data\n",
    "    twitter_df = pd.read_csv('Twitterdataset.csv')\n",
    "    twitter_df['cleaned_tweet'] = twitter_df['tweet'].apply(clean_text)\n",
    "    twitter_df['sentiment'] = twitter_df['cleaned_tweet'].apply(get_sentiment)\n",
    "    twitter_df['processed_content'] = twitter_df['cleaned_tweet'].apply(lemmatize_text)\n",
    "    twitter_df['tweet_length'] = twitter_df['cleaned_tweet'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # Prepare features and target variable\n",
    "    X = twitter_df[['processed_content', 'sentiment', 'tweet_length']]\n",
    "    y = twitter_df['BinaryNumTarget']\n",
    "\n",
    "    # Feature extraction\n",
    "    vector = TfidfVectorizer(max_features=5000)\n",
    "    X_text = vector.fit_transform(X['processed_content'].fillna(' '))\n",
    "    X_final = hstack((X_text, X[['sentiment', 'tweet_length']].values))\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler(with_mean=False)\n",
    "    X_scaled = scaler.fit_transform(X_final)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "    # Hyperparameter tuning with GridSearchCV\n",
    "    param_grid = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear'],  # Focus on solvers that typically converge better\n",
    "        'max_iter': [1000, 5000]  # Increase max_iter to allow for more iterations\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X_train, Y_train)\n",
    "\n",
    "    # Best parameters\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "    # Get the best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Predictions and metrics for training and test sets\n",
    "    logistic_train_pred = best_model.predict(X_train)\n",
    "    logistic_test_pred = best_model.predict(X_test)\n",
    "\n",
    "    print_metrics(Y_train, logistic_train_pred, \"Training\")\n",
    "    print_metrics(Y_test, logistic_test_pred, \"Test\")\n",
    "\n",
    "    # Confusion Matrix for Training Set\n",
    "    cm_train = confusion_matrix(Y_train, logistic_train_pred)\n",
    "    disp_train = ConfusionMatrixDisplay(confusion_matrix=cm_train)\n",
    "    disp_train.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix - Training Set\")\n",
    "    plt.show()\n",
    "\n",
    "    # Confusion Matrix for Test Set\n",
    "    cm_test = confusion_matrix(Y_test, logistic_test_pred)\n",
    "    disp_test = ConfusionMatrixDisplay(confusion_matrix=cm_test)\n",
    "    disp_test.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix - Test Set\")\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve for Training Set\n",
    "    logistic_train_prob = best_model.predict_proba(X_train)[:, 1]\n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(Y_train, logistic_train_prob)\n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(fpr_train, tpr_train, color='green', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_train))\n",
    "    plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve - Training Set')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve for Test Set\n",
    "    logistic_test_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(Y_test, logistic_test_prob)\n",
    "    roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(fpr_test, tpr_test, color='blue', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc_test))\n",
    "    plt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the best model, vectorizer, and scaler\n",
    "    joblib.dump(best_model, \"Best_Logistic_Regression_Model.pkl\")\n",
    "    joblib.dump(vector, \"Vectorizer.pkl\")\n",
    "    joblib.dump(scaler, \"Scaler.pkl\")\n",
    "    print(\"Best model, vectorizer, and scaler are saved.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
